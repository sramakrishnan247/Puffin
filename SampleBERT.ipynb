{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SampleBERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sramakrishnan247/Sentence-Similarity/blob/main/SampleBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGAgzjeUlomz",
        "outputId": "424bf358-cc4f-4998-e55f-db7914abd72f"
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "import transformers"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6NsE8YYl9Eb",
        "outputId": "81ba568f-c68d-4996-e2ed-61e00e441caa"
      },
      "source": [
        "max_length = 128  # Maximum length of input sentence to the model.\n",
        "batch_size = 32\n",
        "epochs = 2\n",
        "\n",
        "# Labels in our dataset.\n",
        "labels = [\"contradiction\", \"entailment\", \"neutral\"]\n",
        "\n",
        "!curl -LO https://raw.githubusercontent.com/MohamadMerchant/SNLI/master/data.tar.gz\n",
        "!tar -xvzf data.tar.gz\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 11.1M  100 11.1M    0     0  41.9M      0 --:--:-- --:--:-- --:--:-- 41.7M\n",
            "SNLI_Corpus/\n",
            "SNLI_Corpus/snli_1.0_dev.csv\n",
            "SNLI_Corpus/snli_1.0_train.csv\n",
            "SNLI_Corpus/snli_1.0_test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7Q6qQvlnCvu",
        "outputId": "bc73c291-58ca-40aa-d978-e2c7ca00f5c0"
      },
      "source": [
        "# There are more than 550k samples in total; we will use 100k for this example.\n",
        "train_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_train.csv\", nrows=100000)\n",
        "valid_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_dev.csv\")\n",
        "test_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_test.csv\")\n",
        "\n",
        "# Shape of the data\n",
        "print(f\"Total train samples : {train_df.shape[0]}\")\n",
        "print(f\"Total validation samples: {valid_df.shape[0]}\")\n",
        "print(f\"Total test samples: {valid_df.shape[0]}\")\n",
        "\n",
        "print(f\"Sentence1: {train_df.loc[1, 'sentence1']}\")\n",
        "print(f\"Sentence2: {train_df.loc[1, 'sentence2']}\")\n",
        "print(f\"Similarity: {train_df.loc[1, 'similarity']}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total train samples : 100000\n",
            "Total validation samples: 10000\n",
            "Total test samples: 10000\n",
            "Sentence1: A person on a horse jumps over a broken down airplane.\n",
            "Sentence2: A person is at a diner, ordering an omelette.\n",
            "Similarity: contradiction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27tVyPB6nVDI",
        "outputId": "3f10981c-a168-46c6-82d1-394877feeb24"
      },
      "source": [
        "# We have some NaN entries in our train data, we will simply drop them.\n",
        "print(\"Number of missing values\")\n",
        "print(train_df.isnull().sum())\n",
        "train_df.dropna(axis=0, inplace=True)\n",
        "print(\"Train Target Distribution\")\n",
        "print(train_df.similarity.value_counts())\n",
        "print(\"Validation Target Distribution\")\n",
        "print(valid_df.similarity.value_counts())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of missing values\n",
            "similarity    0\n",
            "sentence1     0\n",
            "sentence2     3\n",
            "dtype: int64\n",
            "Train Target Distribution\n",
            "entailment       33384\n",
            "contradiction    33310\n",
            "neutral          33193\n",
            "-                  110\n",
            "Name: similarity, dtype: int64\n",
            "Validation Target Distribution\n",
            "entailment       3329\n",
            "contradiction    3278\n",
            "neutral          3235\n",
            "-                 158\n",
            "Name: similarity, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr_9SV11nkxd"
      },
      "source": [
        "train_df = (\n",
        "    train_df[train_df.similarity != \"-\"]\n",
        "    .sample(frac=1.0, random_state=42)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "valid_df = (\n",
        "    valid_df[valid_df.similarity != \"-\"]\n",
        "    .sample(frac=1.0, random_state=42)\n",
        "    .reset_index(drop=True)\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7iuHxAdnoIr"
      },
      "source": [
        "train_df[\"label\"] = train_df[\"similarity\"].apply(\n",
        "    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n",
        ")\n",
        "y_train = tf.keras.utils.to_categorical(train_df.label, num_classes=3)\n",
        "\n",
        "valid_df[\"label\"] = valid_df[\"similarity\"].apply(\n",
        "    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n",
        ")\n",
        "y_val = tf.keras.utils.to_categorical(valid_df.label, num_classes=3)\n",
        "\n",
        "test_df[\"label\"] = test_df[\"similarity\"].apply(\n",
        "    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n",
        ")\n",
        "y_test = tf.keras.utils.to_categorical(test_df.label, num_classes=3)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbQUrPvhpxjK"
      },
      "source": [
        "tokenizer = transformers.BertTokenizer.from_pretrained(\n",
        "            \"bert-base-uncased\", do_lower_case=True\n",
        "        )\n",
        "\n",
        "#tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad', do_lower_case=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkXav6YIqFYG",
        "outputId": "cd9ae63a-031d-4871-8cbf-0e147645e0a6"
      },
      "source": [
        "#Simple training for 10 examples\n",
        "max_length = 128\n",
        "\n",
        "x_train = train_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\")\n",
        "\n",
        "encoded = tokenizer(x_train[0:100000].tolist(), return_tensors='pt', add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True,\n",
        "            pad_to_max_length=True,\n",
        "            )\n",
        "\n",
        "\n",
        "#Embedding and stuff\n",
        "input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
        "\n",
        "x_train = [input_ids, attention_masks, token_type_ids]\n",
        "y_train = tf.keras.utils.to_categorical(train_df[0:100000].label, num_classes=3)\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYLxKtzLl4CG",
        "outputId": "ba54454b-2f19-4e18-fe3c-176aaab284cf"
      },
      "source": [
        "#Creating the model...\n",
        "\n",
        "# Encoded token ids from BERT tokenizer.\n",
        "input_ids = tf.keras.layers.Input(\n",
        "    shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n",
        ")\n",
        "# Attention masks indicates to the model which tokens should be attended to.\n",
        "attention_masks = tf.keras.layers.Input(\n",
        "    shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n",
        ")\n",
        "# Token type ids are binary masks identifying different sequences in the model.\n",
        "token_type_ids = tf.keras.layers.Input(\n",
        "    shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
        ")\n",
        "\n",
        "# Loading pretrained BERT model.\n",
        "bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "#bert_model = transformers.TFBertModel.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "#bert_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "# Freeze the BERT model to reuse the pretrained features without modifying them.\n",
        "bert_model.trainable = False\n",
        "\n",
        "sequence_output, pooled_output = bert_model(\n",
        "  input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
        ")\n",
        "\n",
        "# Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n",
        "bi_lstm = tf.keras.layers.Bidirectional(\n",
        "tf.keras.layers.LSTM(64, return_sequences=True)\n",
        ")(sequence_output)\n",
        "\n",
        "# Applying hybrid pooling approach to bi_lstm sequence output.\n",
        "avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n",
        "max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n",
        "concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
        "dropout = tf.keras.layers.Dropout(0.3)(concat)\n",
        "\n",
        "\n",
        "# sequence_output = tf.keras.layers.Flatten()(sequence_output)\n",
        "output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout)\n",
        "model = tf.keras.models.Model(\n",
        "    inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"acc\"],\n",
        ")\n",
        "model.summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "attention_masks (InputLayer)    [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "token_type_ids (InputLayer)     [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_model_2 (TFBertModel)   ((None, 128, 768), ( 109482240   input_ids[0][0]                  \n",
            "                                                                 attention_masks[0][0]            \n",
            "                                                                 token_type_ids[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 128, 128)     426496      tf_bert_model_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_2 (Glo (None, 128)          0           bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_2 (GlobalM (None, 128)          0           bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 256)          0           global_average_pooling1d_2[0][0] \n",
            "                                                                 global_max_pooling1d_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_113 (Dropout)           (None, 256)          0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 3)            771         dropout_113[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 109,909,507\n",
            "Trainable params: 427,267\n",
            "Non-trainable params: 109,482,240\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1H2nQc3zqEWI",
        "outputId": "233525a7-1c46-4ab8-b6d8-248aa54bca66"
      },
      "source": [
        "model.fit(x_train,y_train,epochs=2,batch_size=16)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "6243/6243 [==============================] - 1056s 169ms/step - loss: 0.6881 - acc: 0.7070\n",
            "Epoch 2/2\n",
            "6243/6243 [==============================] - 1056s 169ms/step - loss: 0.5965 - acc: 0.7576\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fafddb987f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxe2cKpi_EBW",
        "outputId": "e364d5ec-cb02-4771-f9de-9404898d81cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Unfreeze the bert_model.\n",
        "bert_model.trainable = True\n",
        "# Recompile the model to make the change effective.\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "model.summary()\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "attention_masks (InputLayer)    [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "token_type_ids (InputLayer)     [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_model_2 (TFBertModel)   ((None, 128, 768), ( 109482240   input_ids[0][0]                  \n",
            "                                                                 attention_masks[0][0]            \n",
            "                                                                 token_type_ids[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 128, 128)     426496      tf_bert_model_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_2 (Glo (None, 128)          0           bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_2 (GlobalM (None, 128)          0           bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 256)          0           global_average_pooling1d_2[0][0] \n",
            "                                                                 global_max_pooling1d_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_113 (Dropout)           (None, 256)          0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 3)            771         dropout_113[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 109,909,507\n",
            "Trainable params: 109,909,507\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZorgZNn_NjT",
        "outputId": "444de718-c423-4182-b3b8-35960f09ff06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.fit(x_train,y_train,epochs=1,batch_size=16)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "6243/6243 [==============================] - 2841s 455ms/step - loss: 0.4737 - accuracy: 0.8181\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fafdb8bd710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BPxxOOhocit"
      },
      "source": [
        "model.save_weights('BertModel_HundredThousand.h5')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDIirSFZ139j",
        "outputId": "ce80d0e5-09b9-43f9-e728-2a5e31524ee7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6neXPWOLFmSY"
      },
      "source": [
        "!cp BertModel_HundredThousand.h5 drive/MyDrive/EE526/"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6NTT7stcRPC"
      },
      "source": [
        "model2 = tf.keras.models.Model(\n",
        "    inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n",
        ")\n",
        "\n",
        "model2.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"acc\"],\n",
        ")\n",
        "\n",
        "\n",
        "#model2.summary()\n",
        "model2.load_weights('BertModel_weights.h5')\n",
        "#model2.summary()\n",
        "model2.predict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLomgFlG-4ir",
        "outputId": "dfbde06a-66e8-42d1-fc80-4b5c210cef2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def is_similar(sentence1, sentence2):\n",
        "    '''\n",
        "    Takes a sentence1 and checks if sentence2 is symantically similar to sentence1.\n",
        "    '''\n",
        "    sent = [sentence1,sentence2]\n",
        "    \n",
        "    encoded = tokenizer([sent], return_tensors='pt',add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True,\n",
        "            pad_to_max_length=True,\n",
        "            )\n",
        "\n",
        "    input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "    attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "    token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
        "\n",
        "    x_train = [input_ids, attention_masks, token_type_ids]\n",
        "    # y_train = tf.keras.utils.to_categorical(train_df[0].label, num_classes=3)\n",
        "\n",
        "    y_pred = np.array(model.predict(x_train))[0]\n",
        "    print(y_pred)\n",
        "    idx = np.argmax(y_pred)\n",
        "    sentiment_labels = [\"contradiction\", \"entailment\", \"neutral\"]\n",
        "    print(idx)\n",
        "    print(sentiment_labels[idx])\n",
        "    print(y_pred[idx])\n",
        "\n",
        "  \n",
        "is_similar(\"Charlie went to the cycle shop on Sunday\",\"Charlie was resting at home on Sunday\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9.7905320e-01 6.6794438e-04 2.0278854e-02]\n",
            "0\n",
            "contradiction\n",
            "0.9790532\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0og06cni9JsS"
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    }
  ]
}
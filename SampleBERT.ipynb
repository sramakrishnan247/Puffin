{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SampleBERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sramakrishnan247/Sentence-Similarity/blob/main/SampleBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGAgzjeUlomz",
        "outputId": "53c27adb-e56e-4f29-bb19-b9ce5e0df5a1"
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "import transformers"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6NsE8YYl9Eb",
        "outputId": "2cd60bf6-1e42-40cc-933d-0e06c8d20358"
      },
      "source": [
        "max_length = 128  # Maximum length of input sentence to the model.\n",
        "batch_size = 32\n",
        "epochs = 2\n",
        "\n",
        "# Labels in our dataset.\n",
        "labels = [\"contradiction\", \"entailment\", \"neutral\"]\n",
        "\n",
        "!curl -LO https://raw.githubusercontent.com/MohamadMerchant/SNLI/master/data.tar.gz\n",
        "!tar -xvzf data.tar.gz\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 11.1M  100 11.1M    0     0  5624k      0  0:00:02  0:00:02 --:--:-- 5621k\n",
            "SNLI_Corpus/\n",
            "SNLI_Corpus/snli_1.0_dev.csv\n",
            "SNLI_Corpus/snli_1.0_train.csv\n",
            "SNLI_Corpus/snli_1.0_test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7Q6qQvlnCvu",
        "outputId": "94950ef2-9b86-4128-f732-dcd8e720d009"
      },
      "source": [
        "# There are more than 550k samples in total; we will use 100k for this example.\n",
        "train_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_train.csv\", nrows=100000)\n",
        "valid_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_dev.csv\")\n",
        "test_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_test.csv\")\n",
        "\n",
        "# Shape of the data\n",
        "print(f\"Total train samples : {train_df.shape[0]}\")\n",
        "print(f\"Total validation samples: {valid_df.shape[0]}\")\n",
        "print(f\"Total test samples: {valid_df.shape[0]}\")\n",
        "\n",
        "print(f\"Sentence1: {train_df.loc[1, 'sentence1']}\")\n",
        "print(f\"Sentence2: {train_df.loc[1, 'sentence2']}\")\n",
        "print(f\"Similarity: {train_df.loc[1, 'similarity']}\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total train samples : 100000\n",
            "Total validation samples: 10000\n",
            "Total test samples: 10000\n",
            "Sentence1: A person on a horse jumps over a broken down airplane.\n",
            "Sentence2: A person is at a diner, ordering an omelette.\n",
            "Similarity: contradiction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27tVyPB6nVDI",
        "outputId": "7a1fde1f-1d39-4c90-d49d-23adbc21021e"
      },
      "source": [
        "# We have some NaN entries in our train data, we will simply drop them.\n",
        "print(\"Number of missing values\")\n",
        "print(train_df.isnull().sum())\n",
        "train_df.dropna(axis=0, inplace=True)\n",
        "print(\"Train Target Distribution\")\n",
        "print(train_df.similarity.value_counts())\n",
        "print(\"Validation Target Distribution\")\n",
        "print(valid_df.similarity.value_counts())"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of missing values\n",
            "similarity    0\n",
            "sentence1     0\n",
            "sentence2     3\n",
            "dtype: int64\n",
            "Train Target Distribution\n",
            "entailment       33384\n",
            "contradiction    33310\n",
            "neutral          33193\n",
            "-                  110\n",
            "Name: similarity, dtype: int64\n",
            "Validation Target Distribution\n",
            "entailment       3329\n",
            "contradiction    3278\n",
            "neutral          3235\n",
            "-                 158\n",
            "Name: similarity, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr_9SV11nkxd"
      },
      "source": [
        "train_df = (\n",
        "    train_df[train_df.similarity != \"-\"]\n",
        "    .sample(frac=1.0, random_state=42)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "valid_df = (\n",
        "    valid_df[valid_df.similarity != \"-\"]\n",
        "    .sample(frac=1.0, random_state=42)\n",
        "    .reset_index(drop=True)\n",
        ")"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7iuHxAdnoIr"
      },
      "source": [
        "train_df[\"label\"] = train_df[\"similarity\"].apply(\n",
        "    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n",
        ")\n",
        "y_train = tf.keras.utils.to_categorical(train_df.label, num_classes=3)\n",
        "\n",
        "valid_df[\"label\"] = valid_df[\"similarity\"].apply(\n",
        "    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n",
        ")\n",
        "y_val = tf.keras.utils.to_categorical(valid_df.label, num_classes=3)\n",
        "\n",
        "test_df[\"label\"] = test_df[\"similarity\"].apply(\n",
        "    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n",
        ")\n",
        "y_test = tf.keras.utils.to_categorical(test_df.label, num_classes=3)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbQUrPvhpxjK"
      },
      "source": [
        "# tokenizer = transformers.BertTokenizer.from_pretrained(\n",
        "#             \"bert-base-uncased\", do_lower_case=True\n",
        "#         )\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad', do_lower_case=True)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkXav6YIqFYG",
        "outputId": "4fbe4692-32b2-4da1-ddf3-2f5358e67957"
      },
      "source": [
        "#Simple training for 10 examples\n",
        "max_length = 128\n",
        "\n",
        "x_train = train_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\")\n",
        "\n",
        "encoded = tokenizer(x_train[0:100000].tolist(), return_tensors='pt', add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True,\n",
        "            pad_to_max_length=True,\n",
        "            )\n",
        "\n",
        "\n",
        "#Embedding and stuff\n",
        "input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
        "\n",
        "x_train = [input_ids, attention_masks, token_type_ids]\n",
        "y_train = tf.keras.utils.to_categorical(train_df[0:100000].label, num_classes=3)\n",
        "\n"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYLxKtzLl4CG",
        "outputId": "e2d11bdf-95bb-4c4e-e4c9-162207794e08"
      },
      "source": [
        "#Creating the model...\n",
        "\n",
        "# Encoded token ids from BERT tokenizer.\n",
        "input_ids = tf.keras.layers.Input(\n",
        "    shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n",
        ")\n",
        "# Attention masks indicates to the model which tokens should be attended to.\n",
        "attention_masks = tf.keras.layers.Input(\n",
        "    shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n",
        ")\n",
        "# Token type ids are binary masks identifying different sequences in the model.\n",
        "token_type_ids = tf.keras.layers.Input(\n",
        "    shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
        ")\n",
        "\n",
        "# Loading pretrained BERT model.\n",
        "#bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = transformers.TFBertModel.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "#bert_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "# Freeze the BERT model to reuse the pretrained features without modifying them.\n",
        "bert_model.trainable = False\n",
        "\n",
        "sequence_output, pooled_output = bert_model(\n",
        "    input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
        ")\n",
        "sequence_output = tf.keras.layers.Flatten()(sequence_output)\n",
        "output = tf.keras.layers.Dense(3, activation=\"softmax\")(sequence_output)\n",
        "model = tf.keras.models.Model(\n",
        "    inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"acc\"],\n",
        ")\n",
        "model.summary()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing TFBertModel: ['qa_outputs']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "attention_masks (InputLayer)    [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "token_type_ids (InputLayer)     [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_model_4 (TFBertModel)   ((None, 128, 1024),  335141888   input_ids[0][0]                  \n",
            "                                                                 attention_masks[0][0]            \n",
            "                                                                 token_type_ids[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 131072)       0           tf_bert_model_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 3)            393219      flatten_4[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 335,535,107\n",
            "Trainable params: 393,219\n",
            "Non-trainable params: 335,141,888\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1H2nQc3zqEWI",
        "outputId": "a218f868-c352-41d9-a608-9a656fcc03e2"
      },
      "source": [
        "model.fit(x_train,y_train,epochs=2,batch_size=16)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "6243/6243 [==============================] - 3060s 490ms/step - loss: 7.9372 - acc: 0.5417\n",
            "Epoch 2/2\n",
            "6243/6243 [==============================] - 3054s 489ms/step - loss: 8.5762 - acc: 0.5874\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc80f64b7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BPxxOOhocit",
        "outputId": "df82e717-e764-44f4-fe0f-343b03384674"
      },
      "source": [
        "model.save(\"BertModel_1\")"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: BertModel_1/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FZxYNzkpyWN",
        "outputId": "21acd932-b670-4e58-9d4c-e79395e1b24c"
      },
      "source": [
        "print(type(train_df[[\"sentence1\", \"sentence2\"]]))\n",
        "\n",
        "sent = [\"How are you?\",\"Are you doing fine?\"]\n",
        "x_train = train_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\")\n",
        "encoded = tokenizer([sent], return_tensors='pt',add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True,\n",
        "            pad_to_max_length=True,\n",
        "            )\n",
        "\n",
        "input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
        "\n",
        "x_train = [input_ids, attention_masks, token_type_ids]\n",
        "# y_train = tf.keras.utils.to_categorical(train_df[0].label, num_classes=3)\n",
        "\n",
        "#print(x_train)\n",
        "\n",
        "y_pred = np.array(model.predict(x_train))[0]\n",
        "y_pred\n"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.0000000e+00, 4.9040265e-09, 3.5465750e-10], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLomgFlG-4ir"
      },
      "source": [
        "def is_similar(sentence1, sentence2):\n",
        "    '''\n",
        "    Takes a sentence1 and checks if sentence2 is symantically similar to sentence1.\n",
        "    '''\n",
        "    x_train = train_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\")\n",
        "encoded = tokenizer([x_train[15].tolist()], return_tensors='pt',add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True,\n",
        "            pad_to_max_length=True,\n",
        "            )\n",
        "\n",
        "  input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "  attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "  token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
        "\n",
        "  x_train = [input_ids, attention_masks, token_type_ids]\n",
        "  # y_train = tf.keras.utils.to_categorical(train_df[0].label, num_classes=3)\n",
        "\n",
        "  print(x_train)\n",
        "\n",
        "  y_pred = np.array(model.predict(x_train))[0]\n",
        "  y_pred"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ5FRA5n0HWa",
        "outputId": "8ee7c5c9-9ee3-4a41-f4de-9b1f9e037b10"
      },
      "source": [
        "idx = np.argmax(y_pred)\n",
        "\n",
        "sentiment_labels = [\"contradiction\", \"entailment\", \"neutral\"]\n",
        "\n",
        "print(idx)\n",
        "print(sentiment_labels[idx])\n",
        "print(y_pred[idx])\n",
        "print(train_df.label[15])"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "contradiction\n",
            "1.0\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIEqhTWl0KBe",
        "outputId": "94610648-0b76-4c61-fb83-a04babeb3335"
      },
      "source": [
        "np.argmax([1,2,3])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSGR2rEx1yjC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
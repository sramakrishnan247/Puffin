{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReadingComprehensionGame.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPG1gyfuWLAUe0Vr9QLYzvz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sramakrishnan247/Sentence-Similarity/blob/main/ReadingComprehensionGame.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2sIG7nwcz3y"
      },
      "source": [
        "# Reading Comprehension\n",
        "This is an interactive game where a user needs to enter a answer for a given question after reading a paragraph. This is a good way to improve your reading comprehension skills.\n",
        "This game uses a fine-tuned BERT based model that has been trained on the SNLI corpus to compute the semantic similarity. The paragraphs, questions and answers are ranomly generated using SQuAD. There is some boilerplate code that loads the weights, model,etc and the last cell has the actual game implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfHuurckdoM6",
        "outputId": "1376d1a4-af78-482b-8d1c-99e827e808dd"
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "import transformers\n",
        "import random\n",
        "import json\n",
        "import random\n",
        "from pprint import pprint"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1fwbNbLMgxb"
      },
      "source": [
        "# Setup and installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WaSnmFWprGe"
      },
      "source": [
        "### Download the pretrained weights stored in drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nedWm-t6dszH"
      },
      "source": [
        "#Taken from https://github.com/nsadawi/Download-Large-File-From-Google-Drive-Using-Python\n",
        "#taken from this StackOverflow answer: https://stackoverflow.com/a/39225039\n",
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "file_id = '1--jrge8I9VvfeOUuYgJ2AxpkUbKG8jiK'\n",
        "destination = 'weights.h5'\n",
        "download_file_from_google_drive(file_id, destination)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNRo1fbQp3Si"
      },
      "source": [
        "### Create Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLM0EIuDouZN"
      },
      "source": [
        "#Creating the model...\n",
        "def create_pretrained_model():\n",
        "    max_length = 128\n",
        "    # Encoded token ids from BERT tokenizer.\n",
        "    input_ids = tf.keras.layers.Input(\n",
        "        shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n",
        "    )\n",
        "    # Attention masks indicates to the model which tokens should be attended to.\n",
        "    attention_masks = tf.keras.layers.Input(\n",
        "        shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n",
        "    )\n",
        "    # Token type ids are binary masks identifying different sequences in the model.\n",
        "    token_type_ids = tf.keras.layers.Input(\n",
        "        shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
        "    )\n",
        "\n",
        "    # Loading pretrained BERT model.\n",
        "    bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "    # Freeze the BERT model to reuse the pretrained features without modifying them.\n",
        "    bert_model.trainable = False\n",
        "\n",
        "    sequence_output, pooled_output = bert_model(\n",
        "      input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
        "    )\n",
        "\n",
        "    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n",
        "    bi_lstm = tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(64, return_sequences=True)\n",
        "    )(sequence_output)\n",
        "\n",
        "    # Applying hybrid pooling approach to bi_lstm sequence output.\n",
        "    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n",
        "    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n",
        "    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
        "    dropout = tf.keras.layers.Dropout(0.3)(concat)\n",
        "\n",
        "\n",
        "    # sequence_output = tf.keras.layers.Flatten()(sequence_output)\n",
        "    output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout)\n",
        "    model = tf.keras.models.Model(\n",
        "        inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"acc\"],\n",
        "    )\n",
        "    return model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHDlqX5_p7v5"
      },
      "source": [
        "### Load model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ii82pY5reuT7",
        "outputId": "e655fd64-6b44-4188-b1b9-c7c06ec61e9f"
      },
      "source": [
        "model = create_pretrained_model()\n",
        "model.load_weights('weights.h5')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SISGAQa6mtDq",
        "outputId": "098791e7-b51a-4e20-88ad-b9623f6047b1"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "attention_masks (InputLayer)    [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "token_type_ids (InputLayer)     [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_model_1 (TFBertModel)   ((None, 128, 768), ( 109482240   input_ids[0][0]                  \n",
            "                                                                 attention_masks[0][0]            \n",
            "                                                                 token_type_ids[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 128, 128)     426496      tf_bert_model_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 128)          0           bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 128)          0           bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 256)          0           global_average_pooling1d_1[0][0] \n",
            "                                                                 global_max_pooling1d_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_75 (Dropout)            (None, 256)          0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            771         dropout_75[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 109,909,507\n",
            "Trainable params: 427,267\n",
            "Non-trainable params: 109,482,240\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3UE1ji0rBBB"
      },
      "source": [
        "## Similarity Checking "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyTA60KximSW"
      },
      "source": [
        "tokenizer = transformers.BertTokenizer.from_pretrained(\n",
        "          \"bert-base-uncased\", do_lower_case=True\n",
        "      )\n",
        "\n",
        "def is_similar(sentence1, sentence2):\n",
        "    '''\n",
        "    Takes a sentence1 and checks if sentence2 is symantically similar to sentence1.\n",
        "    '''\n",
        "    max_length = 128\n",
        "\n",
        "    sent = [sentence1,sentence2]\n",
        "    \n",
        "    encoded = tokenizer([sent], return_tensors='pt',add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True,\n",
        "            padding='max_length',\n",
        "            )\n",
        "\n",
        "    input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "    attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "    token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
        "\n",
        "    x_train = [input_ids, attention_masks, token_type_ids]\n",
        "    # y_train = tf.keras.utils.to_categorical(train_df[0].label, num_classes=3)\n",
        "\n",
        "    y_pred = np.array(model.predict(x_train))[0]\n",
        "    # print(y_pred)\n",
        "    idx = np.argmax(y_pred)\n",
        "    sentiment_labels = [\"contradiction\", \"entailment\", \"neutral\"]\n",
        "    return (sentiment_labels[idx], y_pred[idx])\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCz3igUh75Pn"
      },
      "source": [
        "## Load SQuAD for generating paras, questions and answers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbU07Kmq5IxC",
        "outputId": "f126f1b5-7b89-4f49-8883-8ec54cb38e4e"
      },
      "source": [
        "!curl -LO https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
        "with open('train-v2.0.json') as f:\n",
        "  dataset = json.load(f)\n",
        "# dataset"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 40.1M  100 40.1M    0     0  52.4M      0 --:--:-- --:--:-- --:--:-- 52.4M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RamZ2tPc7ewJ"
      },
      "source": [
        "# GAME \n",
        "Answer the question after reading the paragraph. The game will keep on running in an infinite loop. If you want to stop playing, hit N after answering the question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKizjceq5LiX",
        "outputId": "9040aba1-8cf2-4ce4-e49b-84ec25039dcb"
      },
      "source": [
        "while(True):\n",
        "    index = random.randint(0,442)\n",
        "    group = dataset['data'][index]\n",
        "\n",
        "    index = random.randint(0,len(group))\n",
        "    paragraph = group['paragraphs'][index]\n",
        "    context = paragraph['context']\n",
        "    question = paragraph['qas'][index]['question']\n",
        "    item = paragraph['qas'][index]\n",
        "    if item['is_impossible']:\n",
        "      answer = paragraph['qas'][index]['plausible_answers'][0]['text']\n",
        "    else:\n",
        "      answer = paragraph['qas'][index]['answers'][0]['text']\n",
        "\n",
        "    print('Read the following paragraph')\n",
        "    print()\n",
        "    pprint(context, width = 95)\n",
        "\n",
        "    print()\n",
        "    print('Answer the following')\n",
        "    print()\n",
        "    print(question)\n",
        "\n",
        "    user_answer = input()\n",
        "    print()\n",
        "    sentiment, similarity = is_similar(answer,user_answer)\n",
        "    if sentiment == 'contradiction':\n",
        "      print('Incorrect answer!!')\n",
        "      print('Correct answer is: ')\n",
        "      print(answer)\n",
        "    elif sentiment == 'entailment':\n",
        "      print('Correct answer!!')\n",
        "      # print('Your answer is ', str(similarity), ' accurate!')\n",
        "    else:\n",
        "      print('Your answer is ', str(similarity), ' accurate!')\n",
        "      print('Correct answer: ')\n",
        "      print(answer)\n",
        "    print('Do you want to coninue: Y/N?')\n",
        "    continue_game = input()\n",
        "    if continue_game not in ['Y','y','yes','Yes']:\n",
        "      break\n",
        "    print()\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read the following paragraph\n",
            "\n",
            "('In 1977, Gaddafi dissolved the Republic and created a new socialist state, the Jamahiriya '\n",
            " '(\"state of the masses\"). Officially adopting a symbolic role in governance, he retained '\n",
            " 'power as military commander-in-chief and head of the Revolutionary Committees responsible '\n",
            " 'for policing and suppressing opponents. Overseeing unsuccessful border conflicts with Egypt '\n",
            " \"and Chad, Gaddafi's support for foreign militants and alleged responsibility for the \"\n",
            " 'Lockerbie bombing led to Libya\\'s label of \"international pariah\". A particularly hostile '\n",
            " 'relationship developed with the United States and United Kingdom, resulting in the 1986 '\n",
            " 'U.S. bombing of Libya and United Nations-imposed economic sanctions. Rejecting his earlier '\n",
            " 'ideological commitments, from 1999 Gaddafi encouraged economic privatization and sought '\n",
            " 'rapprochement with Western nations, also embracing Pan-Africanism and helping to establish '\n",
            " 'the African Union. Amid the Arab Spring, in 2011 an anti-Gaddafist uprising led by the '\n",
            " 'National Transitional Council (NTC) broke out, resulting in the Libyan Civil War. NATO '\n",
            " \"intervened militarily on the side of the NTC, bringing about the government's downfall. \"\n",
            " 'Retreating to Sirte, Gaddafi was captured and killed by NTC militants.')\n",
            "\n",
            "Answer the following\n",
            "\n",
            "List two reasons why Libya become an \"international pariah.\"\n",
            "money and power\n",
            "\n",
            "Correct answer!!\n",
            "Do you want to coninue: Y/N?\n",
            "Y\n",
            "\n",
            "Read the following paragraph\n",
            "\n",
            "('Guinea-Bissau (i/ˈɡɪni bɪˈsaʊ/, GI-nee-bi-SOW), officially the Republic of Guinea-Bissau '\n",
            " '(Portuguese: República da Guiné-Bissau, pronounced: [ʁeˈpublikɐ dɐ ɡiˈnɛ biˈsaw]), is a '\n",
            " 'country in West Africa. It covers 36,125 square kilometres (13,948 sq mi) with an estimated '\n",
            " 'population of 1,704,000.')\n",
            "\n",
            "Answer the following\n",
            "\n",
            "What is the official name for Guinea-Bissau?\n",
            "republic of guniea\n",
            "\n",
            "Correct answer!!\n",
            "Do you want to coninue: Y/N?\n",
            "n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rMCCkBq7iCW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}